{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('batch_size', 256,\n",
    "                            \"\"\"Number of images to process in a batch.\"\"\")\n",
    "tf.app.flags.DEFINE_string('data_dir', '/tmp/comet_dnn_data',\n",
    "                           \"\"\"Path to the comet_dnn data directory.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('use_fp16', False,\n",
    "                            \"\"\"Train the model using fp16.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17565430639775556802"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(np.iinfo(np.uint64).max, dtype='uint64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(10)\n",
    "print(arr[0:1])\n",
    "print(int(1.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for flag, value in FLAGS.__flags.items():\n",
    "    print(flag, \"=\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(n_images, height=18, width=300, depth=2, n_filled=80):\n",
    "    \"\"\"\n",
    "    Generate (n_images * height * width * channels) numpy array with \n",
    "    n_filled randomly filled entries per image.  Note that for a pixel, \n",
    "    both channels are either filled (randomly) or they are both empty.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_images : int \n",
    "        Number of images\n",
    "    height : int\n",
    "        Height of each image\n",
    "    width : int\n",
    "        Width of each image\n",
    "    depth : int\n",
    "        Depth of each image\n",
    "    n_filled : int\n",
    "        Number of pixels filled in each event\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    images : ndarray\n",
    "        Array of shape (n_images * height * width * channels)\n",
    "    \"\"\"\n",
    "    # Initialize the return value\n",
    "    image = np.zeros((n_images, height, width, 2))\n",
    "    # Select around n_filled * n_images channels to fill\n",
    "    layers = np.random.randint(0, high=height-1, size=(n_images, 80))\n",
    "    cells = np.random.randint(0, high=width-1, size=(n_images, 80))\n",
    "    # Fill the channels with random numbers\n",
    "    image[:, layers, cells, :] = np.random.random(size=(n_images, 80,2))\n",
    "    # Cast to 32 bits and return \n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def write_array_to_tfrecord(array, labels, filename, options=None):\n",
    "    # Open TFRecords file, ensure we use gzip compression\n",
    "    writer = tf.python_io.TFRecordWriter(filename, options=options)\n",
    "    \n",
    "    # Write all the images to a file\n",
    "    for lbl, img in zip(labels, array):\n",
    "        # Create a feature\n",
    "        image_as_bytes = tf.train.BytesList(value=[tf.compat.as_bytes(img.tostring())])\n",
    "        label_as_float = tf.train.FloatList(value=[lbl])\n",
    "        feature = {'train/label':  tf.train.Feature(float_list=label_as_float),\n",
    "                   'train/image':  tf.train.Feature(bytes_list=image_as_bytes)}\n",
    "        # Create an example protocol buffer\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        # Serialize to string and write on the file\n",
    "        writer.write(example.SerializeToString())\n",
    "    \n",
    "    # Close the writer and flush the buffer\n",
    "    writer.close()\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def read_tfrecord_to_array(filename, options=None):\n",
    "    feature = {'train/image': tf.FixedLenFeature([], tf.string),\n",
    "               'train/label': tf.FixedLenFeature([], tf.float32)}\n",
    "    # Create a list of filenames and pass it to a queue\n",
    "    filename_queue = tf.train.string_input_producer([filename], num_epochs=1)\n",
    "    # Define a reader and read the next record\n",
    "    reader = tf.TFRecordReader(options=options)\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    # Decode the record read by the reader\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    # Convert the image data from string back to the numbers\n",
    "    image = tf.decode_raw(features['train/image'], tf.float32)\n",
    "    # Cast label data into int32\n",
    "    label = tf.cast(features['train/label'], tf.float32)\n",
    "    # Reshape image data into the original shape\n",
    "    image = tf.reshape(image, [18, 300, 2])\n",
    "\n",
    "    # Any preprocessing here ...\n",
    "\n",
    "    # Creates batches by randomly shuffling tensors\n",
    "    images, labels = tf.train.shuffle_batch([image, label], \n",
    "                                            batch_size=1, \n",
    "                                            capacity=3,\n",
    "                                            num_threads=1, \n",
    "                                            min_after_dequeue=2)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of samples\n",
    "n_random_samples = 10\n",
    "original_images = load_images(n_random_samples)\n",
    "original_labels = np.random.random(n_random_samples)\n",
    "compression = tf.python_io.TFRecordCompressionType.GZIP\n",
    "tf_io_opts = tf.python_io.TFRecordOptions(compression)\n",
    "# Write the file\n",
    "write_array_to_tfrecord(original_images, original_labels, \"train.tfrecords\", tf_io_opts)\n",
    "# Read the files\n",
    "new_images, new_labels = [], []\n",
    "with tf.Session() as sess:\n",
    "    # Get the images and labels\n",
    "    tf_images, tf_labels = read_tfrecord_to_array(\"train.tfrecords\", tf_io_opts)\n",
    "    # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(),\n",
    "                       tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    for batch_index in range(n_random_samples):\n",
    "        img, lbl = sess.run([tf_images, tf_labels])\n",
    "        new_images += [img]\n",
    "        new_labels += [lbl]\n",
    "\n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "# Compare the two arrays\n",
    "np.testing.assert_allclose(original_images, np.vstack(new_images), rtol=1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
